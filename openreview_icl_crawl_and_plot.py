#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OpenReviewï¼ˆICLR/ICMLï¼‰è®ºæ–‡ç»Ÿè®¡å·¥å…·ï¼štitle+abstract æ£€ç´¢ + ç»†åˆ† taxonomy åˆ†ç±» + ä½œå›¾

v4.1 ç‰ˆæœ¬ï¼š
- æ¨¡å—åŒ–è®¾è®¡ï¼šicl_taxonomyï¼ˆåˆ†ç±»ä½“ç³»ï¼‰ã€icl_fetcherï¼ˆæ•°æ®æŠ“å–ï¼‰ã€
  icl_plotterï¼ˆå¯è§†åŒ–ï¼‰ã€icl_classifierï¼ˆåˆ†ç±»å™¨ï¼‰
- æ”¯æŒLLMåˆ†ç±»ï¼šå¯é€‰æ‹©ä½¿ç”¨OpenAIå…¼å®¹çš„APIè¿›è¡Œæ™ºèƒ½åˆ†ç±»
- æ··åˆåˆ†ç±»ç­–ç•¥ï¼šLLM + è§„åˆ™å›é€€ï¼Œæé«˜å‡†ç¡®æ€§
- æ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œç¼“å­˜
- âœ¨ æ–°åŠŸèƒ½ï¼šæ”¯æŒè‡ªå®šä¹‰ä¸»é¢˜å’Œç±»åˆ«ï¼Œçµæ´»é€‚é…ä¸åŒç ”ç©¶é¢†åŸŸ
"""

from __future__ import annotations

import argparse
import os
import sys
from typing import Dict, List

import pandas as pd
from tqdm import tqdm

# å¯¼å…¥æ–°åˆ›å»ºçš„æ¨¡å—
from icl_taxonomy import RuleClassifier
from icl_fetcher import try_fetch_accepted, normalize_note, is_icl_related
from icl_plotter import set_cjk_font, plot_donut_pie, plot_trend
from icl_classifier import LLMClassifier, HybridClassifier
from custom_taxonomy import (
    parse_categories_string, create_custom_taxonomy,
    CustomRuleClassifier, create_topic_filter
)
from config_loader import load_config

VERSION = "v4.2"


def main():
    # åŠ è½½é…ç½®æ–‡ä»¶
    config = load_config()

    ap = argparse.ArgumentParser(
        description="OpenReview è®ºæ–‡ç»Ÿè®¡å·¥å…· v4.2 - æ”¯æŒé…ç½®æ–‡ä»¶å’Œè‡ªå®šä¹‰ä¸»é¢˜",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:

1. ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰:
   ç¼–è¾‘ config.json æ–‡ä»¶ï¼Œç„¶åç›´æ¥è¿è¡Œï¼š
   python3 openreview_icl_crawl_and_plot.py

2. å‘½ä»¤è¡Œè¦†ç›–é…ç½®:
   python3 openreview_icl_crawl_and_plot.py --years 2024 2025 --use_llm

3. è‡ªå®šä¹‰ä¸»é¢˜å’Œç±»åˆ«:
   python3 openreview_icl_crawl_and_plot.py \\
     --topic "multimodal learning" \\
     --categories "è§†è§‰è¯­è¨€èåˆ:vision,language,VLM;è·¨æ¨¡æ€æ£€ç´¢:retrieval,cross-modal"

æ³¨æ„ï¼šå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§é«˜äºé…ç½®æ–‡ä»¶
        """
    )

    # ========== é…ç½®æ–‡ä»¶å‚æ•° ==========
    ap.add_argument("--config", default="config.json",
                    help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤: config.jsonï¼‰")

    # ========== è‡ªå®šä¹‰ä¸»é¢˜å’Œç±»åˆ«å‚æ•° ==========
    ap.add_argument("--topic", default=None,
                    help="è‡ªå®šä¹‰ç ”ç©¶ä¸»é¢˜ï¼ˆç”¨äºè®ºæ–‡è¿‡æ»¤ï¼‰ï¼Œå¦‚ 'multimodal learning'ã€‚ä¸æŒ‡å®šåˆ™ä»é…ç½®æ–‡ä»¶è¯»å–æˆ–ä½¿ç”¨ICLé»˜è®¤ä¸»é¢˜")
    ap.add_argument("--categories", default=None,
                    help="è‡ªå®šä¹‰ç±»åˆ«å®šä¹‰ï¼Œæ ¼å¼: 'ç±»åˆ«1:å…³é”®è¯1,å…³é”®è¯2;ç±»åˆ«2:å…³é”®è¯A,å…³é”®è¯B'ã€‚ä¸æŒ‡å®šåˆ™ä»é…ç½®æ–‡ä»¶è¯»å–æˆ–ä½¿ç”¨ICLé»˜è®¤åˆ†ç±»")

    # ========== åŸºç¡€å‚æ•° ==========
    ap.add_argument("--years", nargs="+", type=int, default=None,
                    help=f"è¦æŠ“å–çš„å¹´ä»½åˆ—è¡¨ï¼ˆé»˜è®¤ä»é…ç½®: {config.years}ï¼‰")
    ap.add_argument("--confs", nargs="+", default=None,
                    help=f"è¦æŠ“å–çš„ä¼šè®®åˆ—è¡¨ï¼ˆé»˜è®¤ä»é…ç½®: {config.conferences}ï¼‰")
    ap.add_argument("--outdir", default=None,
                    help=f"è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä»é…ç½®: {config.output_dir}ï¼‰")
    ap.add_argument("--topk", type=int, default=None,
                    help=f"è¶‹åŠ¿å›¾ä¸­æ˜¾ç¤ºçš„topç±»åˆ«æ•°é‡ï¼ˆé»˜è®¤ä»é…ç½®: {config.topk_trends}ï¼‰")
    ap.add_argument("--timeout", type=int, default=None,
                    help=f"HTTPè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼ˆé»˜è®¤ä»é…ç½®: {config.fetch_timeout}ï¼‰")

    # è¿è¡Œæ¨¡å¼
    ap.add_argument("--quiet", action="store_true", default=None,
                    help="å‡å°‘æ—¥å¿—è¾“å‡º")
    ap.add_argument("--plot_only", action="store_true", default=None,
                    help="ä»…ä»å·²ä¿å­˜çš„CSVç”Ÿæˆå›¾åƒï¼ˆä¸é‡æ–°æŠ“å–ï¼‰")
    ap.add_argument("--data_csv", default=None,
                    help="plot_onlyæ¨¡å¼ä¸‹ä½¿ç”¨çš„CSVè·¯å¾„")

    # å¯è§†åŒ–å‚æ•°
    ap.add_argument("--font", default=None,
                    help=f"æŒ‡å®šMatplotlibå­—ä½“åç§°ï¼ˆé»˜è®¤ä»é…ç½®: {config.font or 'è‡ªåŠ¨'}ï¼‰")

    # LLMåˆ†ç±»å™¨å‚æ•°
    ap.add_argument("--use_llm", action="store_true", default=None,
                    help=f"ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½åˆ†ç±»ï¼ˆé»˜è®¤ä»é…ç½®: {config.use_llm}ï¼‰")
    ap.add_argument("--llm_api_base", default=None,
                    help=f"LLM API base URLï¼ˆé»˜è®¤ä»é…ç½®: {config.api_base}ï¼‰")
    ap.add_argument("--llm_api_key", default=None,
                    help="LLM API keyï¼ˆä¼˜å…ˆçº§ï¼šå‘½ä»¤è¡Œ > é…ç½®æ–‡ä»¶ > ç¯å¢ƒå˜é‡OPENAI_API_KEYï¼‰")
    ap.add_argument("--llm_model", default=None,
                    help=f"LLMæ¨¡å‹åç§°ï¼ˆé»˜è®¤ä»é…ç½®: {config.model}ï¼‰")
    ap.add_argument("--llm_batch_size", type=int, default=None,
                    help=f"LLMæ‰¹å¤„ç†å¤§å°ï¼ˆé»˜è®¤ä»é…ç½®: {config.llm_batch_size}ï¼‰")
    ap.add_argument("--llm_max_rpm", type=int, default=None,
                    help=f"LLM APIæ¯åˆ†é’Ÿæœ€å¤§è¯·æ±‚æ•°ï¼ˆé»˜è®¤ä»é…ç½®: {config.max_rpm}ï¼‰")
    ap.add_argument("--llm_cache_file", default=None,
                    help=f"LLMåˆ†ç±»ç»“æœç¼“å­˜æ–‡ä»¶ï¼ˆé»˜è®¤ä»é…ç½®: {config.cache_file}ï¼‰")
    ap.add_argument("--llm_confidence_threshold", type=float, default=None,
                    help=f"LLMåˆ†ç±»ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆé»˜è®¤ä»é…ç½®: {config.llm_confidence_threshold}ï¼‰")
    ap.add_argument("--checkpoint_file", default=None,
                    help=f"åˆ†ç±»checkpointæ–‡ä»¶ï¼ˆé»˜è®¤ä»é…ç½®: {config.checkpoint_file}ï¼‰")

    args = ap.parse_args()

    # åˆå¹¶é…ç½®ï¼šå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆäºé…ç½®æ–‡ä»¶
    years = args.years if args.years is not None else config.years
    confs = args.confs if args.confs is not None else config.conferences
    outdir = args.outdir if args.outdir is not None else config.output_dir
    topk = args.topk if args.topk is not None else config.topk_trends
    timeout = args.timeout if args.timeout is not None else config.fetch_timeout
    quiet = args.quiet if args.quiet is not None else config.quiet
    plot_only = args.plot_only if args.plot_only is not None else config.plot_only
    data_csv = args.data_csv if args.data_csv else config.data_csv
    font = args.font if args.font is not None else config.font
    use_llm = args.use_llm if args.use_llm is not None else config.use_llm
    llm_api_base = args.llm_api_base if args.llm_api_base is not None else config.api_base
    llm_api_key = args.llm_api_key if args.llm_api_key is not None else config.api_key
    llm_model = args.llm_model if args.llm_model is not None else config.model
    llm_batch_size = args.llm_batch_size if args.llm_batch_size is not None else config.llm_batch_size
    llm_max_rpm = args.llm_max_rpm if args.llm_max_rpm is not None else config.max_rpm
    llm_cache_file = args.llm_cache_file if args.llm_cache_file is not None else config.cache_file
    llm_confidence_threshold = args.llm_confidence_threshold if args.llm_confidence_threshold is not None else config.llm_confidence_threshold
    checkpoint_file = args.checkpoint_file if args.checkpoint_file is not None else config.checkpoint_file
    topic = args.topic if args.topic is not None else config.custom_topic
    categories = args.categories if args.categories is not None else config.custom_categories

    # åˆ›å»ºä¸€ä¸ªåˆå¹¶åçš„argså¯¹è±¡ï¼ˆç”¨äºåç»­ä»£ç å…¼å®¹ï¼‰
    class MergedArgs:
        pass

    merged_args = MergedArgs()
    merged_args.years = years
    merged_args.confs = confs
    merged_args.outdir = outdir
    merged_args.topk = topk
    merged_args.timeout = timeout
    merged_args.quiet = quiet
    merged_args.plot_only = plot_only
    merged_args.data_csv = data_csv
    merged_args.font = font
    merged_args.use_llm = use_llm
    merged_args.llm_api_base = llm_api_base
    merged_args.llm_api_key = llm_api_key
    merged_args.llm_model = llm_model
    merged_args.llm_batch_size = llm_batch_size
    merged_args.llm_max_rpm = llm_max_rpm
    merged_args.llm_cache_file = llm_cache_file
    merged_args.llm_confidence_threshold = llm_confidence_threshold
    merged_args.checkpoint_file = checkpoint_file
    merged_args.topic = topic
    merged_args.categories = categories

    args = merged_args

    os.makedirs(args.outdir, exist_ok=True)
    set_cjk_font(args.font)

    verbose = not args.quiet
    print(f"Running {os.path.basename(__file__)} {VERSION}", flush=True)

    # =============================================================================
    # plot_only æ¨¡å¼ï¼šä»…é‡æ–°ç”Ÿæˆå›¾è¡¨
    # =============================================================================
    if args.plot_only:
        csv_path = args.data_csv or os.path.join(args.outdir, "icl_papers_filtered.csv")
        if not os.path.exists(csv_path):
            print(f"[ERROR] plot_only=True but CSV not found: {csv_path}", file=sys.stderr)
            return
        df = pd.read_csv(csv_path, encoding="utf-8-sig")
        for col in ("year", "category"):
            if col not in df.columns:
                print(f"[ERROR] CSV missing required column: {col}", file=sys.stderr)
                return

        # å¦‚æœcategoryåˆ—æ˜¯category_labelï¼Œéœ€è¦ç¡®è®¤
        if 'category_label' in df.columns and 'category' not in df.columns:
            df['category'] = df['category_label']

        pie_out = os.path.join(args.outdir, "icl_pie_donut_refined.png")
        trend_out = os.path.join(args.outdir, "icl_trend_lines_refined.png")

        confs_str = " & ".join(args.confs)
        year_min, year_max = min(args.years), max(args.years)
        subtitle = "å£å¾„ï¼šOpenReview title+abstractï¼›åˆ†ç±»ï¼šè§„åˆ™æˆ–LLMï¼ˆå¯å¤ç°ï¼‰"

        plot_donut_pie(
            df, pie_out,
            title=f"{confs_str}ï¼ˆ{year_min}â€“{year_max}ï¼‰ICL ç›¸å…³è®ºæ–‡ï¼šç ”ç©¶æ–¹å‘å æ¯”ï¼ˆç»†åˆ† taxonomyï¼‰",
            subtitle=subtitle
        )
        plot_trend(
            df, trend_out,
            title=f"{confs_str}ï¼ˆ{year_min}â€“{year_max}ï¼‰ICL ç›¸å…³è®ºæ–‡ï¼šç ”ç©¶æ–¹å‘å‘æ–‡è¶‹åŠ¿ï¼ˆç»†åˆ† taxonomyï¼‰",
            topk=args.topk,
            years=args.years,
        )

        print("\n[OK] Plot regenerated from saved CSV:")
        print(" -", pie_out)
        print(" -", trend_out)
        return

    # =============================================================================
    # æ•°æ®æŠ“å– + åˆ†ç±» + ç»˜å›¾
    # =============================================================================

    # 1. åˆ¤æ–­æ˜¯å¦ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å¼ï¼ˆåªæœ‰å½“topicå’Œcategorieséƒ½éç©ºæ—¶æ‰å¯ç”¨ï¼‰
    use_custom_mode = (args.topic is not None and args.topic != "") or \
                      (args.categories is not None and args.categories != "")

    if use_custom_mode:
        # éªŒè¯å‚æ•°
        if not args.topic or not args.categories:
            print("[ERROR] --topic and --categories must be used together and cannot be empty", file=sys.stderr)
            print("Example: --topic 'multimodal learning' --categories 'ç±»åˆ«1:kw1,kw2;ç±»åˆ«2:kw3'", file=sys.stderr)
            return

        try:
            # è§£æè‡ªå®šä¹‰ç±»åˆ«
            categories_dict = parse_categories_string(args.categories)
            custom_categories, default_key, default_label = create_custom_taxonomy(categories_dict)

            # åˆ›å»ºä¸»é¢˜è¿‡æ»¤å™¨
            topic_regex, topic_desc = create_topic_filter(args.topic)

            if verbose:
                print(f"\n[Custom Mode] Topic: {args.topic}")
                print(f"[Custom Mode] Defined {len(categories_dict)} categories:")
                for i, (label, keywords) in enumerate(categories_dict.items(), 1):
                    print(f"  {i}. {label}: {', '.join(keywords[:5])}" + ("..." if len(keywords) > 5 else ""))
                print()

            # ä½¿ç”¨è‡ªå®šä¹‰åˆ†ç±»å™¨
            rule_classifier = CustomRuleClassifier(custom_categories, default_key, default_label)

            # è‡ªå®šä¹‰è¿‡æ»¤å‡½æ•°
            def is_topic_related(title: str, abstract: str) -> bool:
                text = f"{title}\n{abstract}"
                return topic_regex.search(text) is not None

            filter_func = is_topic_related
            topic_name = args.topic

        except Exception as e:
            print(f"[ERROR] Failed to parse custom categories: {e}", file=sys.stderr)
            return
    else:
        # é»˜è®¤ICLæ¨¡å¼
        if verbose:
            print(f"\n[Default Mode] Using ICL taxonomy with 9 predefined categories")

        rule_classifier = RuleClassifier()
        filter_func = is_icl_related
        topic_name = "ICL"

    # 2. åˆå§‹åŒ–LLMåˆ†ç±»å™¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
    llm_classifier = None

    if args.use_llm:
        try:
            api_key = args.llm_api_key or os.environ.get("OPENAI_API_KEY")
            if not api_key:
                print("[WARN] --use_llm specified but no API key provided. Using rule-based classification only.",
                      file=sys.stderr)
            else:
                llm_classifier = LLMClassifier(
                    api_base=args.llm_api_base,
                    api_key=api_key,
                    model=args.llm_model,
                    cache_file=os.path.join(args.outdir, args.llm_cache_file),
                    max_rpm=args.llm_max_rpm
                )
                print(f"[LLM] Using {args.llm_model} for classification")
        except ImportError as e:
            print(f"[WARN] Cannot use LLM classifier: {e}. Using rule-based classification.",
                  file=sys.stderr)

    hybrid_classifier = HybridClassifier(
        llm_classifier=llm_classifier,
        rule_classifier=rule_classifier,
        confidence_threshold=args.llm_confidence_threshold
    )

    # 3. æŠ“å–æ•°æ®
    rows: List[Dict] = []
    meta_rows: List[Dict] = []
    all_papers_for_classification = []  # ç”¨äºåˆ†ç±»

    for conf in args.confs:
        for year in args.years:
            try:
                base, inv, sub_n, acc_n, acc_notes = try_fetch_accepted(
                    conf, year, verbose=verbose, timeout=args.timeout
                )
                meta_rows.append({
                    "conf": conf, "year": year,
                    "baseurl": base, "invitation": inv,
                    "submissions": sub_n, "accepted": acc_n
                })
                if verbose:
                    print(f"[{conf} {year}] âœ… accepted fetched: {acc_n} (base={base}, inv={inv})", flush=True)

                # ä½¿ç”¨å¯¹åº”çš„è¿‡æ»¤å‡½æ•°ï¼ˆè‡ªå®šä¹‰æˆ–é»˜è®¤ICLï¼‰
                for n in tqdm(acc_notes, disable=not verbose, desc=f"{conf}-{year} filter"):
                    title, abstract = normalize_note(n)
                    if not filter_func(title, abstract):  # ä½¿ç”¨åŠ¨æ€è¿‡æ»¤å‡½æ•°
                        continue

                    paper_id = f"{conf}_{year}_{n.get('id', len(all_papers_for_classification))}"
                    all_papers_for_classification.append({
                        'id': paper_id,
                        'conf': conf,
                        'year': year,
                        'title': title,
                        'abstract': abstract
                    })

                if verbose:
                    topic_count = sum(1 for p in all_papers_for_classification
                                   if p['conf'] == conf and p['year'] == year)
                    print(f"[{conf} {year}] {topic_name} matched: {topic_count}", flush=True)

            except Exception as e:
                meta_rows.append({
                    "conf": conf, "year": year,
                    "baseurl": "", "invitation": "",
                    "submissions": 0, "accepted": 0,
                    "error": str(e)[:400]
                })
                print(f"[{conf} {year}] âŒ FAILED: {e}", file=sys.stderr, flush=True)

    # ä¿å­˜æŠ“å–å…ƒæ•°æ®
    meta = pd.DataFrame(meta_rows)
    meta_path = os.path.join(args.outdir, "fetch_meta.csv")
    meta.to_csv(meta_path, index=False, encoding="utf-8-sig")

    if not all_papers_for_classification:
        print(f"\nNo papers matched topic '{topic_name}' criteria.", flush=True)
        print(f"See: {meta_path}", flush=True)
        return

    # 4. åˆ†ç±»
    if verbose:
        print(f"\n[Classification] Starting classification for {len(all_papers_for_classification)} papers...")

    checkpoint_file = args.checkpoint_file
    if checkpoint_file:
        checkpoint_file = os.path.join(args.outdir, checkpoint_file)

    classification_results = hybrid_classifier.classify_papers(
        papers=all_papers_for_classification,
        batch_size=args.llm_batch_size,
        checkpoint_file=checkpoint_file,
        verbose=verbose
    )

    # 4. æ•´ç†ç»“æœ
    df = pd.DataFrame(classification_results)

    # ä¿å­˜è¯¦ç»†ç»“æœï¼ˆåŒ…å«æ‰€æœ‰å­—æ®µï¼‰
    output_prefix = "custom" if use_custom_mode else "icl"
    df_detailed_path = os.path.join(args.outdir, f"{output_prefix}_papers_classified_detailed.csv")
    df.to_csv(df_detailed_path, index=False, encoding="utf-8-sig")

    # ä¿å­˜ç®€åŒ–ç‰ˆæœ¬ï¼ˆå…¼å®¹åŸæœ‰æ ¼å¼ï¼‰
    df_simple = df[['conf', 'year', 'title', 'abstract', 'category_label']].copy()
    df_simple.rename(columns={'category_label': 'category'}, inplace=True)
    df_path = os.path.join(args.outdir, f"{output_prefix}_papers_filtered.csv")
    df_simple.to_csv(df_path, index=False, encoding="utf-8-sig")

    # 5. è¾“å‡ºåˆ†ç±»ç»Ÿè®¡
    if verbose:
        print("\n[Classification] Category distribution:")
        print(df['category_label'].value_counts().to_string())
        if 'method' in df.columns:
            print("\n[Classification] Method distribution:")
            print(df['method'].value_counts().to_string())

    # 6. ç»˜å›¾
    pie_out = os.path.join(args.outdir, f"{output_prefix}_pie_donut_refined.png")
    trend_out = os.path.join(args.outdir, f"{output_prefix}_trend_lines_refined.png")

    confs_str = " & ".join(args.confs)
    year_min, year_max = min(args.years), max(args.years)
    method_desc = "LLM+è§„åˆ™æ··åˆ" if args.use_llm and llm_classifier else "è§„åˆ™åŒ¹é…"

    # æ ¹æ®æ¨¡å¼è®¾ç½®æ ‡é¢˜å’Œæ ‡ç­¾
    if use_custom_mode:
        main_title = f"{confs_str}ï¼ˆ{year_min}â€“{year_max}ï¼‰{topic_name} ç›¸å…³è®ºæ–‡"
        subtitle = f"å£å¾„ï¼šOpenReview title+abstractï¼›ä¸»é¢˜ï¼š{topic_name}ï¼›åˆ†ç±»ï¼š{method_desc}"
        center_text = topic_name  # é¥¼å›¾ä¸­å¿ƒæ˜¾ç¤ºä¸»é¢˜å
        legend_title = f"{topic_name}ç ”ç©¶æ–¹å‘åˆ†ç±»"  # å›¾ä¾‹æ ‡é¢˜
        other_label = "å…¶ä»–ï¼ˆé•¿å°¾ï¼‰"  # æŠ˜çº¿å›¾å…¶ä»–ç±»åˆ«æ ‡ç­¾
    else:
        main_title = f"{confs_str}ï¼ˆ{year_min}â€“{year_max}ï¼‰ICL ç›¸å…³è®ºæ–‡"
        subtitle = f"å£å¾„ï¼šOpenReview title+abstractï¼›åˆ†ç±»ï¼š{method_desc}ï¼ˆå¯å¤ç°ï¼‰"
        center_text = "ICLç›¸å…³"  # é¥¼å›¾ä¸­å¿ƒæ˜¾ç¤º"ICLç›¸å…³"
        legend_title = "ç ”ç©¶æ–¹å‘ï¼ˆè§„åˆ™åˆ†ç±»ï¼‰"  # ä¿æŒåŸæœ‰æ ‡é¢˜
        other_label = "ğŸ§º å…¶ä»–ï¼ˆé•¿å°¾ï¼‰"  # ä¿æŒåŸæœ‰emojiæ ‡ç­¾

    # ä½¿ç”¨ç®€åŒ–çš„dfç»˜å›¾
    plot_donut_pie(
        df_simple, pie_out,
        title=f"{main_title}ï¼šç ”ç©¶æ–¹å‘å æ¯”ï¼ˆç»†åˆ† taxonomyï¼‰",
        subtitle=subtitle,
        center_text=center_text,
        legend_title=legend_title
    )
    plot_trend(
        df_simple, trend_out,
        title=f"{main_title}ï¼šç ”ç©¶æ–¹å‘å‘æ–‡è¶‹åŠ¿ï¼ˆç»†åˆ† taxonomyï¼‰",
        topk=args.topk,
        years=args.years,
        other_label=other_label
    )

    print("\nSaved:", flush=True)
    print(" -", df_path)
    print(" -", df_detailed_path)
    print(" -", meta_path)
    print(" -", pie_out)
    print(" -", trend_out)


if __name__ == "__main__":
    main()
