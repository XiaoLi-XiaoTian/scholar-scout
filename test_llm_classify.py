#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯• LLM åˆ†ç±»åŠŸèƒ½ï¼ˆä»é…ç½®æ–‡ä»¶è¯»å– API é…ç½®ï¼‰
"""

import os
import sys

try:
    from icl_taxonomy import RuleClassifier
    from icl_classifier import LLMClassifier, HybridClassifier
    from config_loader import load_config
    print("âœ“ æ¨¡å—å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âœ— æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# ä»é…ç½®æ–‡ä»¶åŠ è½½ API ä¿¡æ¯
config = load_config()
API_CONFIG = {
    "api_base": config.api_base,
    "api_key": config.api_key,
    "model": config.model
}

# æ£€æŸ¥APIå¯†é’¥æ˜¯å¦é…ç½®
if not API_CONFIG["api_key"]:
    print("\nâŒ é”™è¯¯: æœªé…ç½® API å¯†é’¥")
    print("è¯·ç¼–è¾‘ config.json æ–‡ä»¶ï¼Œåœ¨ 'api.api_key' å­—æ®µä¸­å¡«å…¥æ‚¨çš„ API å¯†é’¥")
    sys.exit(1)


def test_llm_classifier():
    """æµ‹è¯• LLM åˆ†ç±»å™¨"""
    print("\n" + "=" * 70)
    print("æµ‹è¯• LLM åˆ†ç±»å™¨")
    print("=" * 70)

    try:
        # åˆå§‹åŒ– LLM åˆ†ç±»å™¨
        print(f"\n[1] åˆå§‹åŒ– LLM åˆ†ç±»å™¨...")
        print(f"    API Base: {API_CONFIG['api_base']}")
        print(f"    Model: {API_CONFIG['model']}")

        llm_classifier = LLMClassifier(
            api_base=API_CONFIG['api_base'],
            api_key=API_CONFIG['api_key'],
            model=API_CONFIG['model'],
            cache_file="test_llm_cache.json",
            max_rpm=10  # é™ä½é€Ÿç‡ä»¥é¿å…è§¦å‘é™åˆ¶
        )
        print("    âœ“ åˆ†ç±»å™¨åˆå§‹åŒ–æˆåŠŸ")

        # å‡†å¤‡æµ‹è¯•è®ºæ–‡
        print("\n[2] å‡†å¤‡æµ‹è¯•è®ºæ–‡...")
        test_papers = [
            {
                'id': 'paper_1',
                'title': 'Chain-of-Thought Prompting Elicits Reasoning in Large Language Models',
                'abstract': 'We explore how generating a chain of thoughtâ€”a series of intermediate reasoning stepsâ€”significantly improves the ability of large language models to perform complex reasoning. We show that chain-of-thought prompting is a simple and broadly applicable technique that can be used with any language model via few-shot in-context learning.'
            },
            {
                'id': 'paper_2',
                'title': 'A Comprehensive Benchmark for In-Context Learning',
                'abstract': 'We present a comprehensive benchmark for evaluating in-context learning capabilities of language models across diverse tasks. Our benchmark includes multiple evaluation metrics and provides detailed analysis of model performance under different in-context learning settings.'
            }
        ]

        for i, paper in enumerate(test_papers, 1):
            print(f"    Paper {i}: {paper['title'][:50]}...")

        # æ‰§è¡Œåˆ†ç±»
        print("\n[3] æ‰§è¡Œ LLM åˆ†ç±»...")
        print("    (è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ...)")

        results = llm_classifier.classify_batch(test_papers, batch_size=2)

        print("    âœ“ åˆ†ç±»å®Œæˆ")

        # æ˜¾ç¤ºç»“æœ
        print("\n[4] åˆ†ç±»ç»“æœ:")
        print("-" * 70)

        for paper, result in zip(test_papers, results):
            print(f"\nğŸ“„ è®ºæ–‡: {paper['title']}")
            print(f"   åˆ†ç±»: {result['category_label']}")
            print(f"   ç½®ä¿¡åº¦: {result['confidence']:.2f}")
            print(f"   ç†ç”±: {result['reasoning']}")
            print(f"   æ¥æº: {'ç¼“å­˜' if result.get('from_cache', False) else 'APIè°ƒç”¨'}")

        print("\n" + "=" * 70)
        print("âœ… LLM åˆ†ç±»å™¨æµ‹è¯•æˆåŠŸï¼")
        print("=" * 70)

        return True

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_hybrid_classifier():
    """æµ‹è¯•æ··åˆåˆ†ç±»å™¨"""
    print("\n" + "=" * 70)
    print("æµ‹è¯•æ··åˆåˆ†ç±»å™¨ï¼ˆLLM + è§„åˆ™å›é€€ï¼‰")
    print("=" * 70)

    try:
        # åˆå§‹åŒ–åˆ†ç±»å™¨
        print("\n[1] åˆå§‹åŒ–æ··åˆåˆ†ç±»å™¨...")

        rule_classifier = RuleClassifier()
        llm_classifier = LLMClassifier(
            api_base=API_CONFIG['api_base'],
            api_key=API_CONFIG['api_key'],
            model=API_CONFIG['model'],
            cache_file="test_llm_cache.json",
            max_rpm=10
        )

        hybrid_classifier = HybridClassifier(
            llm_classifier=llm_classifier,
            rule_classifier=rule_classifier,
            confidence_threshold=0.6
        )
        print("    âœ“ æ··åˆåˆ†ç±»å™¨åˆå§‹åŒ–æˆåŠŸ")

        # å‡†å¤‡æµ‹è¯•è®ºæ–‡
        print("\n[2] å‡†å¤‡æµ‹è¯•è®ºæ–‡...")
        test_papers = [
            {
                'id': 'hybrid_1',
                'title': 'Understanding In-Context Learning Mechanisms',
                'abstract': 'This paper investigates the underlying mechanisms of in-context learning in transformer models through theoretical analysis and empirical experiments.'
            }
        ]

        # æ‰§è¡Œåˆ†ç±»
        print("\n[3] æ‰§è¡Œæ··åˆåˆ†ç±»...")
        results = hybrid_classifier.classify_papers(
            test_papers,
            batch_size=1,
            verbose=False
        )

        # æ˜¾ç¤ºç»“æœ
        print("\n[4] åˆ†ç±»ç»“æœ:")
        print("-" * 70)

        for r in results:
            print(f"\nğŸ“„ è®ºæ–‡: {r['title']}")
            print(f"   åˆ†ç±»: {r['category_label']}")
            print(f"   ç½®ä¿¡åº¦: {r['confidence']:.2f}")
            print(f"   æ–¹æ³•: {r['method']}")
            if 'reasoning' in r:
                print(f"   ç†ç”±: {r['reasoning']}")

        print("\n" + "=" * 70)
        print("âœ… æ··åˆåˆ†ç±»å™¨æµ‹è¯•æˆåŠŸï¼")
        print("=" * 70)

        return True

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    print("\n")
    print("â•”" + "=" * 68 + "â•—")
    print("â•‘" + " " * 15 + "ICL è®ºæ–‡ LLM åˆ†ç±»åŠŸèƒ½æµ‹è¯•" + " " * 24 + "â•‘")
    print("â•š" + "=" * 68 + "â•")

    # æ£€æŸ¥æ˜¯å¦å®‰è£…äº† openai åŒ…
    try:
        import openai
        print(f"âœ“ OpenAI SDK ç‰ˆæœ¬: {openai.__version__}")
    except ImportError:
        print("âŒ é”™è¯¯: æœªå®‰è£… openai åŒ…")
        print("   è¯·è¿è¡Œ: pip install openai")
        sys.exit(1)

    # è¿è¡Œæµ‹è¯•
    success = True

    # æµ‹è¯• 1: LLM åˆ†ç±»å™¨
    if not test_llm_classifier():
        success = False

    # æµ‹è¯• 2: æ··åˆåˆ†ç±»å™¨
    if success:
        if not test_hybrid_classifier():
            success = False

    # æœ€ç»ˆç»“æœ
    print("\n")
    if success:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
        print("\næç¤º:")
        print("  - ç¼“å­˜æ–‡ä»¶å·²ä¿å­˜åˆ°: test_llm_cache.json")
        print("  - ä¸‹æ¬¡è¿è¡Œç›¸åŒè®ºæ–‡å°†ç›´æ¥ä½¿ç”¨ç¼“å­˜")
        print("\nä½¿ç”¨ä¸»ç¨‹åº:")
        print(f"  python3 openreview_icl_crawl_and_plot.py --use_llm")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
        sys.exit(1)


if __name__ == "__main__":
    main()
